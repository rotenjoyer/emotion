<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Emotion Detection</title>
    <style>
        #video {
            width: 100%;
            height: auto;
        }
        #canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: auto;
        }
        #emotion {
            font-size: 30px;
            font-weight: bold;
            position: absolute;
            top: 10px;
            left: 10px;
            z-index: 1;
            color: white;
        }
    </style>
</head>
<body>
    <video id="video" autoplay></video>
    <canvas id="canvas"></canvas>
    <div id="emotion">Loading...</div>
    
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const emotionText = document.getElementById('emotion');

        // Start webcam
        async function startCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;

                video.addEventListener('loadeddata', () => {
                    canvas.width = video.videoWidth;
                    canvas.height = video.videoHeight;
                });
            } catch (err) {
                console.error('Error accessing webcam: ', err);
                emotionText.innerText = 'Error: Cannot access webcam';
            }
        }

        // Load MediaPipe FaceMesh
        async function loadFaceMesh() {
            const faceMesh = new FaceMesh({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}` });
            faceMesh.setOptions({
                maxNumFaces: 1,
                minDetectionConfidence: 0.5,
                minTrackingConfidence: 0.5
            });

            faceMesh.onResults((results) => processResults(results));
            return faceMesh;
        }

        // Process FaceMesh results
        function processResults(results) {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            if (results.multiFaceLandmarks) {
                for (const landmarks of results.multiFaceLandmarks) {
                    drawLandmarks(landmarks);
                    detectEmotion(landmarks);
                }
            }
        }

        // Draw face landmarks & connect key points
        function drawLandmarks(landmarks) {
            ctx.strokeStyle = "lime"; // Make the lines visible (neon green)
            ctx.lineWidth = 3; // Increase thickness for visibility
            ctx.fillStyle = "red"; // Color for landmark points

            // Convert normalized values to actual pixel positions
            const getX = (index) => landmarks[index].x * canvas.width;
            const getY = (index) => landmarks[index].y * canvas.height;

            // Draw dots on landmarks
            landmarks.forEach((point) => {
                ctx.beginPath();
                ctx.arc(point.x * canvas.width, point.y * canvas.height, 3, 0, 2 * Math.PI);
                ctx.fill();
            });

            // Define connections for eyebrows, mouth, and key features
            const connections = [
                [10, 151], // Eyebrows (top of forehead to brow)
                [13, 14],  // Mouth (top lip to bottom lip)
                [48, 54],  // Smile width
                [33, 133], // Eye width
                [61, 291]  // Cheek-to-cheek
            ];

            // Draw lines between connected points
            connections.forEach(([start, end]) => {
                ctx.beginPath();
                ctx.moveTo(getX(start), getY(start));
                ctx.lineTo(getX(end), getY(end));
                ctx.stroke();
            });
        }

        // Emotion detection logic (based on facial features)
        function detectEmotion(landmarks) {
            let emotion = "Neutral ðŸ˜";
            let smile = landmarks[48].x - landmarks[54].x;
            let mouthOpen = landmarks[13].y - landmarks[14].y;
            let browRaise = landmarks[10].y - landmarks[151].y;
            let eyeOpenness = Math.abs(landmarks[159].y - landmarks[145].y); // Eye openness measurement

            // Detect emotions based on facial features
            if (smile > 0.02) {
                emotion = "Happy ðŸ˜Š";
            } else if (browRaise > 0.03) {
                emotion = "Surprised ðŸ˜²";
            } else if (mouthOpen > 0.05) {
                emotion = "Shocked ðŸ˜±";
            } else if (browRaise < -0.02) {
                emotion = "Angry ðŸ˜¡";
            } else if (smile < -0.02) {
                emotion = "Sad ðŸ˜¢"; // Sad Emotion
            } else if (smile < 0.01 && browRaise > 0.01 && mouthOpen < 0.02) {
                emotion = "Nervous ðŸ˜¬"; // Nervous Emotion
            }

            emotionText.innerText = emotion;
        }

        // Initialize everything
        async function init() {
            await startCamera();
            const faceMesh = await loadFaceMesh();
            setInterval(() => faceMesh.send({ image: video }), 100);
        }

        init();
    </script>
</body>
</html>
